{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wangrunqiu/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n",
      "/Users/wangrunqiu/anaconda3/lib/python3.6/site-packages/lightgbm/__init__.py:46: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_9.4.1) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import finishes\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from eli5.sklearn import PermutationImportance\n",
    "from catboost import CatBoostRegressor,Pool\n",
    "import matplotlib.patches as patch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import NuSVR\n",
    "from scipy.stats import kurtosis\n",
    "from scipy.stats import skew\n",
    "from scipy.stats import norm\n",
    "from scipy import linalg\n",
    "from sklearn import tree\n",
    "from sklearn import svm\n",
    "import lightgbm as lgb\n",
    "#import xgboost as xgb\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import graphviz\n",
    "import warnings\n",
    "import random\n",
    "import eli5\n",
    "#import shap  # package used to calculate Shap values\n",
    "import time\n",
    "import glob\n",
    "import sys\n",
    "import os\n",
    "from scipy.signal import hilbert\n",
    "from scipy.signal import hann\n",
    "from scipy.signal import convolve\n",
    "from scipy import stats\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "print(\"Import finishes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading csv file...\n",
      "Train: rows:629145480 columns:2\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"Reading csv file...\")\n",
    "train = pd.read_csv('./train.csv' , dtype={'acoustic_data': np.int16, 'time_to_failure': np.float32})\n",
    "print(\"Train: rows:{} columns:{}\".format(train.shape[0], train.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4194, 1)\n",
      "Reading ends.\n"
     ]
    }
   ],
   "source": [
    "rows = 150_000\n",
    "segments = int(np.floor(train.shape[0] / rows)) # 4194\n",
    "\n",
    "X_train = pd.DataFrame(index=range(segments), dtype=np.float64) # 使用对应的特征\n",
    "\n",
    "y_train = pd.DataFrame(index=range(segments), dtype=np.float64,\n",
    "                       columns=['time_to_failure'])\n",
    "\n",
    "submission = pd.read_csv('./sample_submission.csv', index_col='seg_id')\n",
    "\n",
    "X_train =pd.read_csv('./data/4194samples_137/X_train_138features_filtered.csv',index_col=0)\n",
    "X_test = pd.read_csv('./data/4194samples_137/X_test_138features_filtered.csv',index_col=0)\n",
    "\n",
    "#y_train\n",
    "\n",
    "scale = 1  # test:使滑动窗口缩小10倍\n",
    "for segment in range(segments):\n",
    "    # y_train and X_train\n",
    "    #     if segment * rows / scale + rows > train.shape[0]:\n",
    "    #         break\n",
    "    seg = train.iloc[int(segment * rows / scale): int(segment * rows / scale + rows)]\n",
    "    x = pd.Series(seg['acoustic_data'].values)\n",
    "    y = seg['time_to_failure'].values[-1]\n",
    "    y_train.loc[segment, 'time_to_failure'] = y\n",
    "#print(X_train.head())\n",
    "#print(X_test.head())\n",
    "print(y_train.shape)\n",
    "#print(\"submission has: rows:{} cols:{}\".format(submission.shape[0], submission.shape[1]))\n",
    "print(\"Reading ends.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FillNA and standardScalar...\n",
      "(4194, 137)\n",
      "FillNA and standardScalar ends.\n"
     ]
    }
   ],
   "source": [
    "# 正则化\n",
    "print(\"FillNA and standardScalar...\")\n",
    "means_dict = {}\n",
    "for col in X_train.columns:\n",
    "    if X_train[col].isnull().any():\n",
    "        print(col)\n",
    "        mean_value = X_train.loc[X_train[col] != -np.inf, col].mean()\n",
    "        X_train.loc[X_train[col] == -np.inf, col] = mean_value\n",
    "        X_train[col] = X_train[col].fillna(mean_value)\n",
    "        means_dict[col] = mean_value\n",
    "\n",
    "for col in X_test.columns:\n",
    "    if X_test[col].isnull().any():\n",
    "        X_test.loc[X_test[col] == -np.inf, col] = means_dict[col]\n",
    "        X_test[col] = X_test[col].fillna(means_dict[col])\n",
    "\n",
    "X = X_train.copy()\n",
    "y = y_train.copy()\n",
    "#train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = pd.DataFrame(scaler.transform(X_train), columns=X_train.columns)\n",
    "X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
    "print(X_train_scaled.shape)\n",
    "print(\"FillNA and standardScalar ends.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "{'learning_rate': 0.05, 'max_bin': 60, 'max_depth': 1, 'n_estimators': 100}\n",
      "end\n"
     ]
    }
   ],
   "source": [
    "print('start')\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_test2 = {'n_estimators':range(100,1000,100),'learning_rate':[0.01,0.02,0.03,0.04,0.05],'max_depth':range(1,5),'max_bin':range(50,300,10)}\n",
    "gsearch2 = GridSearchCV(estimator = lgb.LGBMRegressor(), param_grid = param_test2, scoring=None,cv=5)\n",
    "gsearch2.fit(X_train_scaled,y_train)\n",
    "print(gsearch2.best_params_)\n",
    "submission['time_to_failure'] = gsearch2.predict(X_test_scaled)\n",
    "submission.to_csv('submission_lgb1.csv')\n",
    "print('end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
