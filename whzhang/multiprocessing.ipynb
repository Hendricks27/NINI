{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import warnings\n",
    "import traceback\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import scipy.signal as sg\n",
    "import multiprocessing as mp\n",
    "from scipy.signal import hann\n",
    "from scipy.signal import hilbert\n",
    "from scipy.signal import convolve\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from tqdm import tqdm\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = r'd:\\NINI data\\final_model'  # set for local environment\n",
    "DATA_DIR = r'd:\\NINI data\\data'  # set for local environment\n",
    "\n",
    "SIG_LEN = 150000\n",
    "NUM_SEG_PER_PROC = 4000\n",
    "NUM_THREADS = 6\n",
    "\n",
    "NY_FREQ_IDX = 75000  # the test signals are 150k samples long, Nyquist is thus 75k.\n",
    "CUTOFF = 18000\n",
    "MAX_FREQ_IDX = 20000\n",
    "FREQ_STEP = 2500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_raw_data():\n",
    "    df = pd.read_csv(os.path.join(DATA_DIR, 'train.csv'))\n",
    "\n",
    "    max_start_index = len(df.index) - SIG_LEN\n",
    "    slice_len = int(max_start_index / 6)\n",
    "\n",
    "    for i in range(NUM_THREADS):\n",
    "        print('working', i)\n",
    "        df0 = df.iloc[slice_len * i: (slice_len * (i + 1)) + SIG_LEN]\n",
    "        df0.to_csv(os.path.join(DATA_DIR, 'raw_data_%d.csv' % i), index=False)\n",
    "        del df0\n",
    "\n",
    "    del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rnd_idxs():\n",
    "    rnd_idxs = np.zeros(shape=(NUM_THREADS, NUM_SEG_PER_PROC), dtype=np.int32)\n",
    "    max_start_idx = 100000000\n",
    "\n",
    "    for i in range(NUM_THREADS):\n",
    "        np.random.seed(5591 + i)\n",
    "        start_indices = np.random.randint(0, max_start_idx, size=NUM_SEG_PER_PROC, dtype=np.int32)\n",
    "        rnd_idxs[i, :] = start_indices\n",
    "\n",
    "    for i in range(NUM_THREADS):\n",
    "        print(rnd_idxs[i, :8])\n",
    "        print(rnd_idxs[i, -8:])\n",
    "        print(min(rnd_idxs[i,:]), max(rnd_idxs[i,:]))\n",
    "\n",
    "    np.savetxt(fname=os.path.join(OUTPUT_DIR, 'start_indices_4k.csv'), X=np.transpose(rnd_idxs), fmt='%d', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_trend_feature(arr, abs_values=False):\n",
    "    idx = np.array(range(len(arr)))\n",
    "    if abs_values:\n",
    "        arr = np.abs(arr)\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(idx.reshape(-1, 1), arr)\n",
    "    return lr.coef_[0]\n",
    "\n",
    "def classic_sta_lta(x, length_sta, length_lta):\n",
    "    sta = np.cumsum(x ** 2)\n",
    "    # Convert to float\n",
    "    sta = np.require(sta, dtype=np.float)\n",
    "    # Copy for LTA\n",
    "    lta = sta.copy()\n",
    "    # Compute the STA and the LTA\n",
    "    sta[length_sta:] = sta[length_sta:] - sta[:-length_sta]\n",
    "    sta /= length_sta\n",
    "    lta[length_lta:] = lta[length_lta:] - lta[:-length_lta]\n",
    "    lta /= length_lta\n",
    "    # Pad zeros\n",
    "    sta[:length_lta - 1] = 0\n",
    "    # Avoid division by zero by setting zero values to tiny float\n",
    "    dtiny = np.finfo(0.0).tiny\n",
    "    idx = lta < dtiny\n",
    "    lta[idx] = dtiny\n",
    "    return sta / lta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def des_bw_filter_lp(cutoff=CUTOFF):  # low pass filter\n",
    "    b, a = sg.butter(4, Wn=cutoff/NY_FREQ_IDX)\n",
    "    return b, a\n",
    "\n",
    "def des_bw_filter_hp(cutoff=CUTOFF):  # high pass filter\n",
    "    b, a = sg.butter(4, Wn=cutoff/NY_FREQ_IDX, btype='highpass')\n",
    "    return b, a\n",
    "\n",
    "def des_bw_filter_bp(low, high):  # band pass filter\n",
    "    b, a = sg.butter(4, Wn=(low/NY_FREQ_IDX, high/NY_FREQ_IDX), btype='bandpass')\n",
    "    return b, a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(seg_id, seg, X, st, end):\n",
    "    try:\n",
    "        X.loc[seg_id, 'seg_id'] = np.int32(seg_id)\n",
    "        X.loc[seg_id, 'seg_start'] = np.int32(st)\n",
    "        X.loc[seg_id, 'seg_end'] = np.int32(end)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    xc = pd.Series(seg['acoustic_data'].values)\n",
    "    xcdm = xc - np.mean(xc)\n",
    "\n",
    "    b, a = des_bw_filter_lp(cutoff=18000)\n",
    "    xcz = sg.lfilter(b, a, xcdm)\n",
    "\n",
    "    zc = np.fft.fft(xcz)\n",
    "    zc = zc[:MAX_FREQ_IDX]\n",
    "\n",
    "    # FFT transform values\n",
    "    realFFT = np.real(zc)\n",
    "    imagFFT = np.imag(zc)\n",
    "\n",
    "    freq_bands = [x for x in range(0, MAX_FREQ_IDX, FREQ_STEP)]\n",
    "    magFFT = np.sqrt(realFFT ** 2 + imagFFT ** 2)\n",
    "    phzFFT = np.arctan(imagFFT / realFFT)\n",
    "    phzFFT[phzFFT == -np.inf] = -np.pi / 2.0\n",
    "    phzFFT[phzFFT == np.inf] = np.pi / 2.0\n",
    "    phzFFT = np.nan_to_num(phzFFT)\n",
    "\n",
    "    for freq in freq_bands:\n",
    "        X.loc[seg_id, 'FFT_Mag_01q%d' % freq] = np.quantile(magFFT[freq: freq + FREQ_STEP], 0.01)\n",
    "        X.loc[seg_id, 'FFT_Mag_10q%d' % freq] = np.quantile(magFFT[freq: freq + FREQ_STEP], 0.1)\n",
    "        X.loc[seg_id, 'FFT_Mag_90q%d' % freq] = np.quantile(magFFT[freq: freq + FREQ_STEP], 0.9)\n",
    "        X.loc[seg_id, 'FFT_Mag_99q%d' % freq] = np.quantile(magFFT[freq: freq + FREQ_STEP], 0.99)\n",
    "        X.loc[seg_id, 'FFT_Mag_mean%d' % freq] = np.mean(magFFT[freq: freq + FREQ_STEP])\n",
    "        X.loc[seg_id, 'FFT_Mag_std%d' % freq] = np.std(magFFT[freq: freq + FREQ_STEP])\n",
    "        X.loc[seg_id, 'FFT_Mag_max%d' % freq] = np.max(magFFT[freq: freq + FREQ_STEP])\n",
    "\n",
    "        X.loc[seg_id, 'FFT_Phz_mean%d' % freq] = np.mean(phzFFT[freq: freq + FREQ_STEP])\n",
    "        X.loc[seg_id, 'FFT_Phz_std%d' % freq] = np.std(phzFFT[freq: freq + FREQ_STEP])\n",
    "\n",
    "    X.loc[seg_id, 'FFT_Rmean'] = realFFT.mean()\n",
    "    X.loc[seg_id, 'FFT_Rstd'] = realFFT.std()\n",
    "    X.loc[seg_id, 'FFT_Rmax'] = realFFT.max()\n",
    "    X.loc[seg_id, 'FFT_Rmin'] = realFFT.min()\n",
    "    X.loc[seg_id, 'FFT_Imean'] = imagFFT.mean()\n",
    "    X.loc[seg_id, 'FFT_Istd'] = imagFFT.std()\n",
    "    X.loc[seg_id, 'FFT_Imax'] = imagFFT.max()\n",
    "    X.loc[seg_id, 'FFT_Imin'] = imagFFT.min()\n",
    "\n",
    "    X.loc[seg_id, 'FFT_Rmean_first_6000'] = realFFT[:6000].mean()\n",
    "    X.loc[seg_id, 'FFT_Rstd__first_6000'] = realFFT[:6000].std()\n",
    "    X.loc[seg_id, 'FFT_Rmax_first_6000'] = realFFT[:6000].max()\n",
    "    X.loc[seg_id, 'FFT_Rmin_first_6000'] = realFFT[:6000].min()\n",
    "    X.loc[seg_id, 'FFT_Rmean_first_18000'] = realFFT[:18000].mean()\n",
    "    X.loc[seg_id, 'FFT_Rstd_first_18000'] = realFFT[:18000].std()\n",
    "    X.loc[seg_id, 'FFT_Rmax_first_18000'] = realFFT[:18000].max()\n",
    "    X.loc[seg_id, 'FFT_Rmin_first_18000'] = realFFT[:18000].min()\n",
    "\n",
    "    del xcz\n",
    "    del zc\n",
    "\n",
    "    b, a = des_bw_filter_lp(cutoff=2500)\n",
    "    xc0 = sg.lfilter(b, a, xcdm)\n",
    "\n",
    "    b, a = des_bw_filter_bp(low=2500, high=5000)\n",
    "    xc1 = sg.lfilter(b, a, xcdm)\n",
    "\n",
    "    b, a = des_bw_filter_bp(low=5000, high=7500)\n",
    "    xc2 = sg.lfilter(b, a, xcdm)\n",
    "\n",
    "    b, a = des_bw_filter_bp(low=7500, high=10000)\n",
    "    xc3 = sg.lfilter(b, a, xcdm)\n",
    "\n",
    "    b, a = des_bw_filter_bp(low=10000, high=12500)\n",
    "    xc4 = sg.lfilter(b, a, xcdm)\n",
    "\n",
    "    b, a = des_bw_filter_bp(low=12500, high=15000)\n",
    "    xc5 = sg.lfilter(b, a, xcdm)\n",
    "\n",
    "    b, a = des_bw_filter_bp(low=15000, high=17500)\n",
    "    xc6 = sg.lfilter(b, a, xcdm)\n",
    "\n",
    "    b, a = des_bw_filter_bp(low=17500, high=20000)\n",
    "    xc7 = sg.lfilter(b, a, xcdm)\n",
    "\n",
    "    b, a = des_bw_filter_hp(cutoff=20000)\n",
    "    xc8 = sg.lfilter(b, a, xcdm)\n",
    "\n",
    "    sigs = [xc, pd.Series(xc0), pd.Series(xc1), pd.Series(xc2), pd.Series(xc3),\n",
    "            pd.Series(xc4), pd.Series(xc5), pd.Series(xc6), pd.Series(xc7), pd.Series(xc8)]\n",
    "\n",
    "    for i, sig in enumerate(sigs):\n",
    "        X.loc[seg_id, 'mean_%d' % i] = sig.mean()\n",
    "        X.loc[seg_id, 'std_%d' % i] = sig.std()\n",
    "        X.loc[seg_id, 'max_%d' % i] = sig.max()\n",
    "        X.loc[seg_id, 'min_%d' % i] = sig.min()\n",
    "\n",
    "        X.loc[seg_id, 'mean_change_abs_%d' % i] = np.mean(np.diff(sig))\n",
    "        X.loc[seg_id, 'mean_change_rate_%d' % i] = np.mean(np.nonzero((np.diff(sig) / sig[:-1]))[0])\n",
    "        X.loc[seg_id, 'abs_max_%d' % i] = np.abs(sig).max()\n",
    "        X.loc[seg_id, 'abs_min_%d' % i] = np.abs(sig).min()\n",
    "\n",
    "        X.loc[seg_id, 'std_first_50000_%d' % i] = sig[:50000].std()\n",
    "        X.loc[seg_id, 'std_last_50000_%d' % i] = sig[-50000:].std()\n",
    "        X.loc[seg_id, 'std_first_10000_%d' % i] = sig[:10000].std()\n",
    "        X.loc[seg_id, 'std_last_10000_%d' % i] = sig[-10000:].std()\n",
    "\n",
    "        X.loc[seg_id, 'avg_first_50000_%d' % i] = sig[:50000].mean()\n",
    "        X.loc[seg_id, 'avg_last_50000_%d' % i] = sig[-50000:].mean()\n",
    "        X.loc[seg_id, 'avg_first_10000_%d' % i] = sig[:10000].mean()\n",
    "        X.loc[seg_id, 'avg_last_10000_%d' % i] = sig[-10000:].mean()\n",
    "\n",
    "        X.loc[seg_id, 'min_first_50000_%d' % i] = sig[:50000].min()\n",
    "        X.loc[seg_id, 'min_last_50000_%d' % i] = sig[-50000:].min()\n",
    "        X.loc[seg_id, 'min_first_10000_%d' % i] = sig[:10000].min()\n",
    "        X.loc[seg_id, 'min_last_10000_%d' % i] = sig[-10000:].min()\n",
    "\n",
    "        X.loc[seg_id, 'max_first_50000_%d' % i] = sig[:50000].max()\n",
    "        X.loc[seg_id, 'max_last_50000_%d' % i] = sig[-50000:].max()\n",
    "        X.loc[seg_id, 'max_first_10000_%d' % i] = sig[:10000].max()\n",
    "        X.loc[seg_id, 'max_last_10000_%d' % i] = sig[-10000:].max()\n",
    "\n",
    "        X.loc[seg_id, 'max_to_min_%d' % i] = sig.max() / np.abs(sig.min())\n",
    "        X.loc[seg_id, 'max_to_min_diff_%d' % i] = sig.max() - np.abs(sig.min())\n",
    "        X.loc[seg_id, 'count_big_%d' % i] = len(sig[np.abs(sig) > 500])\n",
    "        X.loc[seg_id, 'sum_%d' % i] = sig.sum()\n",
    "\n",
    "        X.loc[seg_id, 'mean_change_rate_first_50000_%d' % i] = np.mean(np.nonzero((np.diff(sig[:50000]) / sig[:50000][:-1]))[0])\n",
    "        X.loc[seg_id, 'mean_change_rate_last_50000_%d' % i] = np.mean(np.nonzero((np.diff(sig[-50000:]) / sig[-50000:][:-1]))[0])\n",
    "        X.loc[seg_id, 'mean_change_rate_first_10000_%d' % i] = np.mean(np.nonzero((np.diff(sig[:10000]) / sig[:10000][:-1]))[0])\n",
    "        X.loc[seg_id, 'mean_change_rate_last_10000_%d' % i] = np.mean(np.nonzero((np.diff(sig[-10000:]) / sig[-10000:][:-1]))[0])\n",
    "\n",
    "        X.loc[seg_id, 'q95_%d' % i] = np.quantile(sig, 0.95)\n",
    "        X.loc[seg_id, 'q99_%d' % i] = np.quantile(sig, 0.99)\n",
    "        X.loc[seg_id, 'q05_%d' % i] = np.quantile(sig, 0.05)\n",
    "        X.loc[seg_id, 'q01_%d' % i] = np.quantile(sig, 0.01)\n",
    "\n",
    "        X.loc[seg_id, 'abs_q95_%d' % i] = np.quantile(np.abs(sig), 0.95)\n",
    "        X.loc[seg_id, 'abs_q99_%d' % i] = np.quantile(np.abs(sig), 0.99)\n",
    "        X.loc[seg_id, 'abs_q05_%d' % i] = np.quantile(np.abs(sig), 0.05)\n",
    "        X.loc[seg_id, 'abs_q01_%d' % i] = np.quantile(np.abs(sig), 0.01)\n",
    "\n",
    "        X.loc[seg_id, 'trend_%d' % i] = add_trend_feature(sig)\n",
    "        X.loc[seg_id, 'abs_trend_%d' % i] = add_trend_feature(sig, abs_values=True)\n",
    "        X.loc[seg_id, 'abs_mean_%d' % i] = np.abs(sig).mean()\n",
    "        X.loc[seg_id, 'abs_std_%d' % i] = np.abs(sig).std()\n",
    "\n",
    "        X.loc[seg_id, 'mad_%d' % i] = sig.mad()\n",
    "        X.loc[seg_id, 'kurt_%d' % i] = sig.kurtosis()\n",
    "        X.loc[seg_id, 'skew_%d' % i] = sig.skew()\n",
    "        X.loc[seg_id, 'med_%d' % i] = sig.median()\n",
    "\n",
    "        X.loc[seg_id, 'Hilbert_mean_%d' % i] = np.abs(hilbert(sig)).mean()\n",
    "        X.loc[seg_id, 'Hann_window_mean'] = (convolve(xc, hann(150), mode='same') / sum(hann(150))).mean()\n",
    "\n",
    "        X.loc[seg_id, 'classic_sta_lta1_mean_%d' % i] = classic_sta_lta(sig, 500, 10000).mean()\n",
    "        X.loc[seg_id, 'classic_sta_lta2_mean_%d' % i] = classic_sta_lta(sig, 5000, 100000).mean()\n",
    "        X.loc[seg_id, 'classic_sta_lta3_mean_%d' % i] = classic_sta_lta(sig, 3333, 6666).mean()\n",
    "        X.loc[seg_id, 'classic_sta_lta4_mean_%d' % i] = classic_sta_lta(sig, 10000, 25000).mean()\n",
    "\n",
    "        X.loc[seg_id, 'Moving_average_700_mean_%d' % i] = sig.rolling(window=700).mean().mean(skipna=True)\n",
    "        X.loc[seg_id, 'Moving_average_1500_mean_%d' % i] = sig.rolling(window=1500).mean().mean(skipna=True)\n",
    "        X.loc[seg_id, 'Moving_average_3000_mean_%d' % i] = sig.rolling(window=3000).mean().mean(skipna=True)\n",
    "        X.loc[seg_id, 'Moving_average_6000_mean_%d' % i] = sig.rolling(window=6000).mean().mean(skipna=True)\n",
    "\n",
    "        ewma = pd.Series.ewm\n",
    "        X.loc[seg_id, 'exp_Moving_average_300_mean_%d' % i] = ewma(sig, span=300).mean().mean(skipna=True)\n",
    "        X.loc[seg_id, 'exp_Moving_average_3000_mean_%d' % i] = ewma(sig, span=3000).mean().mean(skipna=True)\n",
    "        X.loc[seg_id, 'exp_Moving_average_30000_mean_%d' % i] = ewma(sig, span=6000).mean().mean(skipna=True)\n",
    "\n",
    "        no_of_std = 2\n",
    "        X.loc[seg_id, 'MA_700MA_std_mean_%d' % i] = sig.rolling(window=700).std().mean()\n",
    "        X.loc[seg_id, 'MA_700MA_BB_high_mean_%d' % i] = (\n",
    "                    X.loc[seg_id, 'Moving_average_700_mean_%d' % i] + no_of_std * X.loc[seg_id, 'MA_700MA_std_mean_%d' % i]).mean()\n",
    "        X.loc[seg_id, 'MA_700MA_BB_low_mean_%d' % i] = (\n",
    "                    X.loc[seg_id, 'Moving_average_700_mean_%d' % i] - no_of_std * X.loc[seg_id, 'MA_700MA_std_mean_%d' % i]).mean()\n",
    "        X.loc[seg_id, 'MA_400MA_std_mean_%d' % i] = sig.rolling(window=400).std().mean()\n",
    "        X.loc[seg_id, 'MA_400MA_BB_high_mean_%d' % i] = (\n",
    "                    X.loc[seg_id, 'Moving_average_700_mean_%d' % i] + no_of_std * X.loc[seg_id, 'MA_400MA_std_mean_%d' % i]).mean()\n",
    "        X.loc[seg_id, 'MA_400MA_BB_low_mean_%d' % i] = (\n",
    "                    X.loc[seg_id, 'Moving_average_700_mean_%d' % i] - no_of_std * X.loc[seg_id, 'MA_400MA_std_mean_%d' % i]).mean()\n",
    "        X.loc[seg_id, 'MA_1000MA_std_mean_%d' % i] = sig.rolling(window=1000).std().mean()\n",
    "\n",
    "        X.loc[seg_id, 'iqr_%d' % i] = np.subtract(*np.percentile(sig, [75, 25]))\n",
    "        X.loc[seg_id, 'q999_%d' % i] = np.quantile(sig, 0.999)\n",
    "        X.loc[seg_id, 'q001_%d' % i] = np.quantile(sig, 0.001)\n",
    "        X.loc[seg_id, 'ave10_%d' % i] = stats.trim_mean(sig, 0.1)\n",
    "\n",
    "    for windows in [10, 100, 1000]:\n",
    "        x_roll_std = xc.rolling(windows).std().dropna().values\n",
    "        x_roll_mean = xc.rolling(windows).mean().dropna().values\n",
    "\n",
    "        X.loc[seg_id, 'ave_roll_std_' + str(windows)] = x_roll_std.mean()\n",
    "        X.loc[seg_id, 'std_roll_std_' + str(windows)] = x_roll_std.std()\n",
    "        X.loc[seg_id, 'max_roll_std_' + str(windows)] = x_roll_std.max()\n",
    "        X.loc[seg_id, 'min_roll_std_' + str(windows)] = x_roll_std.min()\n",
    "        X.loc[seg_id, 'q01_roll_std_' + str(windows)] = np.quantile(x_roll_std, 0.01)\n",
    "        X.loc[seg_id, 'q05_roll_std_' + str(windows)] = np.quantile(x_roll_std, 0.05)\n",
    "        X.loc[seg_id, 'q95_roll_std_' + str(windows)] = np.quantile(x_roll_std, 0.95)\n",
    "        X.loc[seg_id, 'q99_roll_std_' + str(windows)] = np.quantile(x_roll_std, 0.99)\n",
    "        X.loc[seg_id, 'av_change_abs_roll_std_' + str(windows)] = np.mean(np.diff(x_roll_std))\n",
    "        X.loc[seg_id, 'av_change_rate_roll_std_' + str(windows)] = np.mean(\n",
    "            np.nonzero((np.diff(x_roll_std) / x_roll_std[:-1]))[0])\n",
    "        X.loc[seg_id, 'abs_max_roll_std_' + str(windows)] = np.abs(x_roll_std).max()\n",
    "\n",
    "        X.loc[seg_id, 'ave_roll_mean_' + str(windows)] = x_roll_mean.mean()\n",
    "        X.loc[seg_id, 'std_roll_mean_' + str(windows)] = x_roll_mean.std()\n",
    "        X.loc[seg_id, 'max_roll_mean_' + str(windows)] = x_roll_mean.max()\n",
    "        X.loc[seg_id, 'min_roll_mean_' + str(windows)] = x_roll_mean.min()\n",
    "        X.loc[seg_id, 'q01_roll_mean_' + str(windows)] = np.quantile(x_roll_mean, 0.01)\n",
    "        X.loc[seg_id, 'q05_roll_mean_' + str(windows)] = np.quantile(x_roll_mean, 0.05)\n",
    "        X.loc[seg_id, 'q95_roll_mean_' + str(windows)] = np.quantile(x_roll_mean, 0.95)\n",
    "        X.loc[seg_id, 'q99_roll_mean_' + str(windows)] = np.quantile(x_roll_mean, 0.99)\n",
    "        X.loc[seg_id, 'av_change_abs_roll_mean_' + str(windows)] = np.mean(np.diff(x_roll_mean))\n",
    "        X.loc[seg_id, 'av_change_rate_roll_mean_' + str(windows)] = np.mean(\n",
    "            np.nonzero((np.diff(x_roll_mean) / x_roll_mean[:-1]))[0])\n",
    "        X.loc[seg_id, 'abs_max_roll_mean_' + str(windows)] = np.abs(x_roll_mean).max()\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_fields(proc_id):\n",
    "    success = 1\n",
    "    count = 0\n",
    "    try:\n",
    "        seg_st = int(NUM_SEG_PER_PROC * proc_id)\n",
    "        train_df = pd.read_csv(os.path.join(DATA_DIR, 'raw_data_%d.csv' % proc_id), dtype={'acoustic_data': np.int16, 'time_to_failure': np.float32})\n",
    "        len_df = len(train_df.index)\n",
    "        start_indices = (np.loadtxt(fname=os.path.join(OUTPUT_DIR, 'start_indices_4k.csv'), dtype=np.int32, delimiter=','))[:, proc_id]\n",
    "        train_X = pd.DataFrame(dtype=np.float64)\n",
    "        train_y = pd.DataFrame(dtype=np.float64, columns=['time_to_failure'])\n",
    "        t0 = time.time()\n",
    "\n",
    "        for seg_id, start_idx in zip(range(seg_st, seg_st + NUM_SEG_PER_PROC), start_indices):\n",
    "            end_idx = np.int32(start_idx + 150000)\n",
    "            print('working: %d, %d, %d to %d of %d' % (proc_id, seg_id, start_idx, end_idx, len_df))\n",
    "            seg = train_df.iloc[start_idx: end_idx]\n",
    "            # train_X = create_features_pk_det(seg_id, seg, train_X, start_idx, end_idx)\n",
    "            train_X = create_features(seg_id, seg, train_X, start_idx, end_idx)\n",
    "            train_y.loc[seg_id, 'time_to_failure'] = seg['time_to_failure'].values[-1]\n",
    "\n",
    "            if count == 10: \n",
    "                print('saving: %d, %d to %d' % (seg_id, start_idx, end_idx))\n",
    "                train_X.to_csv('train_x_%d.csv' % proc_id, index=False)\n",
    "                train_y.to_csv('train_y_%d.csv' % proc_id, index=False)\n",
    "\n",
    "            count += 1\n",
    "\n",
    "        print('final_save, process id: %d, loop time: %.2f for %d iterations' % (proc_id, time.time() - t0, count))\n",
    "        train_X.to_csv(os.path.join(OUTPUT_DIR, 'train_x_%d.csv' % proc_id), index=False)\n",
    "        train_y.to_csv(os.path.join(OUTPUT_DIR, 'train_y_%d.csv' % proc_id), index=False)\n",
    "\n",
    "    except:\n",
    "        print(traceback.format_exc())\n",
    "        success = 0\n",
    "\n",
    "    return success  # 1 on success, 0 if fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mp_build():\n",
    "    t0 = time.time()\n",
    "    num_proc = NUM_THREADS\n",
    "    pool = mp.Pool(processes=num_proc)\n",
    "    results = [pool.apply_async(build_fields, args=(pid, )) for pid in range(NUM_THREADS)]\n",
    "    output = [p.get() for p in results]\n",
    "    num_built = sum(output)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    print(num_built)\n",
    "    print('Run time: %.2f' % (time.time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_mp_build():\n",
    "    df0 = pd.read_csv(os.path.join(OUTPUT_DIR, 'train_x_%d.csv' % 0))\n",
    "    df1 = pd.read_csv(os.path.join(OUTPUT_DIR, 'train_y_%d.csv' % 0))\n",
    "\n",
    "    for i in range(1, NUM_THREADS):\n",
    "        print('working %d' % i)\n",
    "        temp = pd.read_csv(os.path.join(OUTPUT_DIR, 'train_x_%d.csv' % i))\n",
    "        df0 = df0.append(temp)\n",
    "\n",
    "        temp = pd.read_csv(os.path.join(OUTPUT_DIR, 'train_y_%d.csv' % i))\n",
    "        df1 = df1.append(temp)\n",
    "\n",
    "    df0.to_csv(os.path.join(OUTPUT_DIR, 'train_x.csv'), index=False)\n",
    "    df1.to_csv(os.path.join(OUTPUT_DIR, 'train_y.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_test_fields():\n",
    "    train_X = pd.read_csv(os.path.join(OUTPUT_DIR, 'train_x.csv'))\n",
    "    try:\n",
    "        train_X.drop(labels=['seg_id', 'seg_start', 'seg_end'], axis=1, inplace=True)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    submission = pd.read_csv(os.path.join(DATA_DIR, 'sample_submission.csv'), index_col='seg_id')\n",
    "    test_X = pd.DataFrame(columns=train_X.columns, dtype=np.float64, index=submission.index)\n",
    "\n",
    "    print('start for loop')\n",
    "    count = 0\n",
    "    for seg_id in tqdm_notebook(test_X.index):  # just tqdm in IDE\n",
    "        seg = pd.read_csv(os.path.join(DATA_DIR, 'test', str(seg_id) + '.csv'))\n",
    "        # train_X = create_features_pk_det(seg_id, seg, train_X, start_idx, end_idx)\n",
    "        test_X = create_features(seg_id, seg, test_X, 0, 0)\n",
    "\n",
    "        if count % 100 == 0:\n",
    "            print('working', seg_id)\n",
    "        count += 1\n",
    "\n",
    "    test_X.to_csv(os.path.join(OUTPUT_DIR, 'test_x.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_fields(fn_train='train_x.csv', fn_test='test_x.csv', \n",
    "                 fn_out_train='scaled_train_X.csv' , fn_out_test='scaled_test_X.csv'):\n",
    "    train_X = pd.read_csv(os.path.join(OUTPUT_DIR, fn_train))\n",
    "    try:\n",
    "        train_X.drop(labels=['seg_id', 'seg_start', 'seg_end'], axis=1, inplace=True)\n",
    "    except:\n",
    "        pass\n",
    "    test_X = pd.read_csv(os.path.join(OUTPUT_DIR, fn_test))\n",
    "\n",
    "    print('start scaler')\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(train_X)\n",
    "    scaled_train_X = pd.DataFrame(scaler.transform(train_X), columns=train_X.columns)\n",
    "    scaled_test_X = pd.DataFrame(scaler.transform(test_X), columns=test_X.columns)\n",
    "\n",
    "    scaled_train_X.to_csv(os.path.join(OUTPUT_DIR, fn_out_train), index=False)\n",
    "    scaled_test_X.to_csv(os.path.join(OUTPUT_DIR, fn_out_test), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-cae9d31a4422>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msplit_raw_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-1c7b8297d33b>\u001b[0m in \u001b[0;36msplit_raw_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msplit_raw_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'train.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mmax_start_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mSIG_LEN\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mslice_len\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_start_index\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 446\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    447\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m         \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1034\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'skipfooter not supported for iteration'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1035\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1036\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1037\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m         \u001b[1;31m# May alter columns / col_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1846\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1847\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1849\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\dtypes\\common.py\u001b[0m in \u001b[0;36mis_integer_dtype\u001b[1;34m(arr_or_dtype)\u001b[0m\n\u001b[0;32m    809\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m \u001b[1;32mdef\u001b[0m \u001b[0mis_integer_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    812\u001b[0m     \"\"\"\n\u001b[0;32m    813\u001b[0m     \u001b[0mCheck\u001b[0m \u001b[0mwhether\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mprovided\u001b[0m \u001b[0marray\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mof\u001b[0m \u001b[0man\u001b[0m \u001b[0minteger\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "split_raw_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_rnd_idxs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_mp_build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_mp_build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_test_fields()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_fields()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
